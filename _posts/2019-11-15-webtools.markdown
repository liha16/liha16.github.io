---
layout: post
title:  "Web tools"
date:   2019-11-17 04:48:03 -0600
categories: webtools update
comments: true
image: /img/avatar.gif
---
 
### CSS
_What do you think of pre-compiling your CSS?_
* _Compare to regular CSS_
* _Which techniques did you use?_
* _Pros and cons?_

I like the fact that CCS pre processors use variables and nesting to organize the code better and to be DRY. Normally, if you want to change one color that you use on many places in your css file you have to change the code in many places. With variables it has to be changed only at one place, very practical! I made a few more variables to use more colors and fonts. I don't find it enough useful to change from normal CSS. Normal CSS is a large file and sometimes a bit hard to find hat you're looking for but there are ways to divide the style rules (divide into different documents). Using it togehter with Jekyll wasn't easier that to use regular CSS. I would have prefered to edit a regular CSS document. Aso because it's easier to use the development toools in the webbrowser the regular way.

 
### SSG
_What do you think of static site generators?
What type of projects are they suitable for?_

I think they can be very useful if you want to make a little web site fast, specially if you want to make a blog that you only administrate yourself as it doesn't allow you have a database. 
I have already made my own template in html and I use php to include documents as head, header and footer. I use this template to set up basic websites fast. And I find it much easier to edit and customize that Jekyll. Jeykyll has a lot of files and maybe not all developers are familiar with the structure so if a site needs to be updated by a developer with no experience with jekyll it more complicated than a basic html using php. 



### Robots.txt
_What is robots.txt and how have you configure it for your site?_

Robots.txt is used to give robots(computers) instructions of how to use your website and is read before reading the rest of the web site. 
The file has to be placed in the top directory of the site and should be named robots.txt. The file tells robots which files they can access on the web site and not. Normally robots are search engines but they can also be malware and the last mentioned wont consider the robots.txt instructions. On this site I am disallowing all robots cause I don't have any interest in getting indexed etc.

### Humans.txt
_What is humans.txt and how have you configure it for your site?_

Humans.txt is a document with information about who is behind a site, who created and contributed in one way or another. Is should also be located in the top directory of the site. 

### Comments
_How did you implements comments to blog posts?_

I used disqus which was easy no install even though it took me a while to understand exaclty where the code had to be placed. I placed a script tag in _layout/default.html and another script in the _layout/post.html file after creating an account on disqus. It is simple and working well.

### Open Graph
_What is Open Graph and how do you make use of it?_

Open Graph is a way to control how the website is presented when sharing it on social media. You can effect this presentation by editing tags in the head of the hmtl document and changing the information, for example: og:image and a adress to the image. I changed the image (globally) to my avatar image made in the parallel course and in the blog post about Barcelona I choose a picture of Barcelona, only for that post. I had to edit the head.html file to insert og:type as I couldn't find any documentation about how to do it in an other way. Inserting it in _config didn't work.